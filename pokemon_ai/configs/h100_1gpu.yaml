# Configuration for single H100 GPU - MAXIMUM SPEED
# 200M param model on 3.9M trajectories

model:
  size: "base"  # ~200M parameters - matches Metamon paper
  use_flash_attention: true  # 2-3x faster attention
  use_gradient_checkpointing: true  # Enable to reduce GPU memory usage

data:
  path: "data/replays"
  max_turns: 100
  formats: ["gen1ou", "gen2ou", "gen3ou", "gen4ou", "gen5ou", "gen6ou", "gen7ou", "gen8ou", "gen9ou"]
  # Data is on RAM disk (/dev/shm) - no need to preload, lazy loading is fast
  preload_to_ram: false

training:
  batch_size: 32  # Reduced to fit in GPU memory with gradient checkpointing
  gradient_accumulation_steps: 4  # Effective batch = 128 (same as before)
  num_epochs: 3
  max_steps: null
  learning_rate: 3e-4
  warmup_steps: 500
  seed: 42
  # Compile model for faster training (PyTorch 2.0+)
  # Disabled: causes dtype issues with Flash Attention
  compile_model: false

rl:
  actor_method: "binary"
  actor_coef: 1.0
  critic_coef: 0.5
  entropy_coef: 0.01

optimization:
  use_mixed_precision: true
  mixed_precision_dtype: "bf16"  # H100 native bf16

deepspeed:
  enabled: false  # Not needed for single GPU, adds overhead

logging:
  log_interval: 100
  eval_interval: 1000
  save_interval: 2000
  use_wandb: true
  wandb_project: "pokemon-superhuman"
  wandb_run_name: "h100-phase1-fast"

checkpoint:
  output_dir: "checkpoints/h100_base"
  save_total_limit: 3

hardware:
  num_workers: 12  # More workers = faster data loading
  pin_memory: true  # Faster CPU->GPU transfer
  prefetch_factor: 4  # Prefetch more batches
